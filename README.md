
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align="center">
  <h1 style="font-size: 4rem; font-weight: bold; color: #667eea; margin: 20px 0; display: flex; align-items: center; justify-content: center; gap: 20px;">
    <!-- <img src="assets/tsail_rdt.png" alt="TSAIL RDT" style="height: 8rem; width: auto;" /> -->
    RDT2: é€šè¿‡æ‰©å±• UMI æ•°æ®å®ç°é›¶æ ·æœ¬è·¨æœ¬ä½“æ³›åŒ–
  </h1>
</div>
<!-- <hr> -->
<div align="center" style="line-height: 1;">
  <a href="https://rdt-robotics.github.io/rdt2/"><img alt="Homepage"
    src="https://img.shields.io/badge/RDT%202-Homepage-4287f5?logo=probot&logoColor=#009BD5"/></a>
  <a href="https://huggingface.co/collections/robotics-diffusion-transformer/rdt-2-68ce9ddbf7dc520a231220d5"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-TSAIL%20RDT-ffc107?color=ffc107&logoColor=white"/></a>
  <!-- <br> -->
  <a href="https://discord.gg/vsZS3zmf9A"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-RDT-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <br>
<a href="https://rdt-robotics.github.io/rdt2/feishu.html"><img alt="Feishu"
    src="https://img.shields.io/badge/Feishu-RDT-blue?logo=lark&logoColor=white"/></a>
  <a href="https://x.com/songming_liu/status/1971643908372550108"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-RDT-white?logo=x&logoColor=white"/></a>
  <!-- <br>
  <a href="LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-Apache--2.0-f5de53?logo=apache&color=f5de53"/></a>
  <!-- <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a> -->
  <!-- <br> -->
  <!-- <a href="https://arxiv.org/pdf/2412.19437"><b>Blog Link</b>ğŸ‘ï¸</a>  -->
  <a href="https://arxiv.org/abs/2602.03310"><img alt="Paper"
    src="https://img.shields.io/badge/arXiv-Paper-B31B1B?logo=arxiv"/></a>
  <!-- <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a> -->
  <br>
  <!-- <a href=""><b>Paper Link</b>ğŸ“„</a> -->
</div>

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Overview](#overview)
- [Updates](#updates)
- [Requirements](#requirements)
- [Installation](#installation)
- [Model Checkpoints](#model-checkpoints)
- [Running Inference for a Pre-Trained Model](#running-inference-for-a-pre-trained-model)
  - [1. \[IMPORTANT\] Hard-ware Set up and Calibration](#1-important-hard-ware-set-up-and-calibration)
  - [2. Run Inference](#2-run-inference)
- [Fine-Tuning Models on Your Own Data](#fine-tuning-models-on-your-own-data)
  - [1. Convert your data to WebDataset shards](#1-convert-your-data-to-webdataset-shards)
  - [2. Defining training configs and running training](#2-defining-training-configs-and-running-training)
  - [3. Run training](#3-run-training)
    - [RDT2-VQ](#rdt2-vq)
    - [RDT2-FM](#rdt2-fm)
  - [ç²¾åº¦è®¾ç½®](#ç²¾åº¦è®¾ç½®)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## æ¦‚è¿°

RDT2 æ˜¯ [RDT-1B](https://rdt-robotics.github.io/rdt-robotics/) çš„ç»­ä½œï¼Œæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨**æœªè§è¿‡çš„æœºå™¨äººæœ¬ä½“**ä¸Šå®ç°**é›¶æ ·æœ¬éƒ¨ç½²**çš„**ç®€å•å¼€æ”¾è¯æ±‡**ä»»åŠ¡ï¼ˆå¦‚æŠ“å–ã€æ”¾ç½®ã€æ‘‡æ™ƒã€æ“¦æ‹­ç­‰ï¼‰çš„åŸºç¡€æ¨¡å‹ã€‚è¿™ä¸€é‡Œç¨‹ç¢‘çš„å®ç°å¾—ç›Šäºå¤šæ–¹é¢çš„åŠªåŠ›ï¼š

- æˆ‘ä»¬é€šè¿‡é‡‡ç”¨æ›´é«˜å¼ºåº¦çš„ææ–™å’Œæ›´ç²¾ç¡®çš„è·Ÿè¸ªæ–¹æ³•é‡æ–°è®¾è®¡äº† [UMI ç¡¬ä»¶](https://umi-gripper.github.io)ï¼Œç¡®ä¿å…¶åœ¨å¤§è§„æ¨¡æ•°æ®æ”¶é›†ä¸­å…·æœ‰å¯é æ€§ã€‚
- æˆ‘ä»¬åœ¨**100+ ä¸ªä¸åŒçš„å®¤å†…åœºæ™¯**ä¸­æ”¶é›†äº†**è¶…è¿‡ 10,000 å°æ—¶**çš„äººç±»æ“ä½œè§†é¢‘ï¼Œæ¶µç›–äº†å¤¹çˆªå¯ä»¥æ‰§è¡Œçš„å¤§éƒ¨åˆ†å®¶åº­ä»»åŠ¡ã€‚

ç›®å‰ï¼Œæœ¬ä»“åº“åŒ…å«ä»¥ä¸‹æ¨¡å‹ï¼š
- [RDT2-VQ](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ)ï¼šä¸€ä¸ªè‡ªåŠ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ï¼Œé‡‡ç”¨ [Residual VQ](https://arxiv.org/abs/2107.03312) ä½œä¸ºåŠ¨ä½œæ ‡è®°å™¨ï¼ŒåŸºäº [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) å¹¶ä½¿ç”¨æˆ‘ä»¬çš„ UMI æ•°æ®é›†è¿›è¡Œé€‚é…ï¼Œå®ç°äº†å“è¶Šçš„é›¶æ ·æœ¬æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚
- [RDT2-FM](https://huggingface.co/robotics-diffusion-transformer/RDT2-FM)ï¼šä¸€ä¸ªæ”¹è¿›çš„ RDT æ¨¡å‹ï¼Œä½œä¸ºåŠ¨ä½œä¸“å®¶ï¼Œé‡‡ç”¨æµåŒ¹é…ï¼ˆflow-matchingï¼‰ç›®æ ‡å‡½æ•°ï¼Œæ¨ç†å»¶è¿Ÿæ˜¾è‘—é™ä½ã€‚

å¯¹äºæ‰€æœ‰æ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†æ£€æŸ¥ç‚¹å’Œç¤ºä¾‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–åŸºäºæ‚¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚ç›®å‰ï¼Œæˆ‘ä»¬å·²åœ¨åŒ…æ‹¬ [åŒè‡‚ UR5e](https://www.universal-robots.com/products/ur5e/) å’Œ [åŒè‡‚ Franka Research 3](https://franka.de/franka-research-3) åœ¨å†…çš„å¹³å°ä¸ŠéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä¹è§‚åœ°è®¤ä¸ºï¼Œé€šè¿‡éµå¾ªæˆ‘ä»¬çš„[æŒ‡å—](#è¿è¡Œé¢„è®­ç»ƒæ¨¡å‹æ¨ç†)ï¼Œæœªæ¥å¯ä»¥åœ¨æ›´å¤šå¹³å°ä¸ŠæˆåŠŸéƒ¨ç½²è¿™äº›æ¨¡å‹ã€‚


## æ›´æ–°æ—¥å¿—

- [Sept 2025] We released [RDT2-VQ](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ) \& [RDT2-FM](https://huggingface.co/robotics-diffusion-transformer/RDT2-FM), the sequel of RDT-1B with better open-world generalization and zero-shot deployment on unseen embodiments.
- [Feb 2026] We released the [arXiv](https://arxiv.org/abs/2602.03310) paper.

## Requirements

To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism or offload into CPU to reduce per-GPU memory. Since RDT2 is based on Qwen2.5-VL-7B, you basiclly need to follow the hard-ware requirements for Qwen2.5-VL-7B:

| Mode               | RAM Required | VRAM Required | Example GPU        |
| ------------------ | --------------- | --------------- | ------------------ |
| æ¨ç†          | > 32 GB      | ~ 16 GB | RTX 4090           |
| å¾®è°ƒ RDT2-FM (RDT ä¸“å®¶) |   -     | ~ 16 GB | RTX 4090           |
| å¾®è°ƒ RDT2-VQ (LoRA) |   -     | > 32 GB | A100 (40GB)           |
| å¾®è°ƒ RDT2-VQ (å…¨å‚æ•°) |   -    |  > 80 GB  | A100 (80GB) / H100 / B200|

å¯¹äºé›¶æ ·æœ¬éƒ¨ç½²ï¼Œæ‚¨éœ€è¦è´­ä¹°æŒ‡å®šçš„*æœ«ç«¯æ‰§è¡Œå™¨*å’Œ*ç›¸æœº*ï¼Œå¹¶æ ¹æ®[ç¡¬ä»¶è®¾ç½®ä¸æ ‡å®š](#1-é‡è¦-ç¡¬ä»¶è®¾ç½®ä¸æ ‡å®š)è¿›è¡Œ 3D æ‰“å°ç›¸åº”çš„*ç›¸æœºæ”¯æ¶*å’Œ*æ³•å…°*ã€‚

æœ¬ä»“åº“å·²åœ¨ Ubuntu 24.04 ä¸Šæµ‹è¯•ï¼Œæˆ‘ä»¬ç›®å‰ä¸æ”¯æŒå…¶ä»–æ“ä½œç³»ç»Ÿã€‚

## å®‰è£…

å…‹éš†æœ¬ä»“åº“å¹¶åˆ›å»º conda ç¯å¢ƒï¼š

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/thu-ml/RDT2.git
cd RDT2

# åˆ›å»º conda ç¯å¢ƒ
conda create -n rdt2 python=3.10 -y
conda activate rdt2

# å®‰è£… torch (cuda12.8)
pip install torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu128

# å®‰è£… flash attention,å¯èƒ½æŠ¥é”™ç¼ºå°‘psutilåŒ… ç›´æ¥pip installå³å¯
pip install -U psutil
pip install flash-attn --no-build-isolation

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt

# å‡çº§ nvidia-nccl-cu12
pip install --upgrade --force-reinstall nvidia-nccl-cu12==2.27.5

# å†æ¬¡ç¡®è®¤å·²å®‰è£…æ­£ç¡®çš„ transformers 4.51.3
pip list | grep transformers

# éƒ¨ç½²åˆ° UR5e
pip install -r requirements/ur5e.txt

# éƒ¨ç½²åˆ° Franka Research 3
pip install -r requirements/franka_research_3.txt
```
ä¸‹è½½å¯¹åº”çš„å½’ä¸€åŒ–æ–‡ä»¶
```bash
http://ml.cs.tsinghua.edu.cn/~lingxuan/rdt2/umi_normalizer_wo_downsample_indentity_rot.pt
```
## æ¨¡å‹æ£€æŸ¥ç‚¹

<!-- ###  Models -->
æˆ‘ä»¬æä¾›äº†å¤šä¸ª VLA æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œèƒ½å¤Ÿåœ¨å„ç§æœºå™¨äººå¹³å°å’Œç®€å•è¯æ±‡ä»»åŠ¡ä¸Šéƒ¨ç½²ã€‚å¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æœºå™¨äººå¹³å°ä¸Šä½¿ç”¨å…¶ä»–æœ«ç«¯æ‰§è¡Œå™¨å’Œç›¸æœºè¿›è¡Œéƒ¨ç½²ï¼Œå¯ä»¥ä»åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚


| æ¨¡å‹        | ä½¿ç”¨åœºæ™¯    | æè¿°                                                                                                 | æ£€æŸ¥ç‚¹è·¯å¾„                                |
| ------------ | ----------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| normalizer      | Inference & Fine-Tuning (Freeze) | Normalizer for action normalization   | [umi_normalizer_wo_downsample_indentity_rot.pt](https://huggingface.co/robotics-diffusion-transformer/RVQActionTokenizer/blob/main/umi_normalizer_wo_downsample_indentity_rot.pt)    |
| Residual VQ  | Inference & Fine-Tuning (Freeze) |  Residual VQ (RVQ) as the action tokenizer   | [`robotics-diffusion-transformer/RVQActionTokenizer`](https://huggingface.co/robotics-diffusion-transformer/RVQActionTokenizer)    |
| RDT2-VQ      | Inference & Fine-Tuning | Auto-regressive VLA with Residual VQ as the action tokenizer   | [`robotics-diffusion-transformer/RDT2-VQ`](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ)    |
| RDT2-FM      | Inference & Fine-Tuning | Auto-regressive VLA (RDT2-VQ) with Flow-Matching Action Expert   | [`robotics-diffusion-transformer/RDT2-FM`](https://huggingface.co/robotics-diffusion-transformer/RDT2-FM)    |

<!-- | $\pi_0$-FAST | Fine-Tuning | Base autoregressive [Ï€â‚€-FAST model](https://www.physicalintelligence.company/research/fast) for fine-tuning | `gs://openpi-assets/checkpoints/pi0_fast_base` |
|| $\pi_{0.5}$    | Fine-Tuning | Base [Ï€â‚€.â‚… model](https://www.physicalintelligence.company/blog/pi05) for fine-tuning    | `gs://openpi-assets/checkpoints/pi05_base`      | -->

<!-- ### Fine-Tuned Models -->


<!-- | Model                    | Use Case    | Description                                                                                                                                                                                              | Checkpoint Path                                       |
|| ------------------------ | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
|| $\pi_0$-FAST-DROID       | Inference   | $\pi_0$-FAST model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform | `gs://openpi-assets/checkpoints/pi0_fast_droid`       |
|| $\pi_0$-DROID            | Fine-Tuning | $\pi_0$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/): faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well                                | `gs://openpi-assets/checkpoints/pi0_droid`            |
|| $\pi_0$-ALOHA-towel      | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can fold diverse towels 0-shot on ALOHA robot platforms                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_towel`      |
|| $\pi_0$-ALOHA-tupperware | Inference   | $\pi_0$ model fine-tuned on internal [ALOHA](https://tonyzhaozh.github.io/aloha/) data: can unpack food from a tupperware container                                                                                                             | `gs://openpi-assets/checkpoints/pi0_aloha_tupperware` |
|| $\pi_0$-ALOHA-pen-uncap  | Inference   | $\pi_0$ model fine-tuned on public [ALOHA](https://dit-policy.github.io/) data: can uncap a pen                                                                                                          | `gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap`  |
|| $\pi_{0.5}$-LIBERO      | Inference   | $\pi_{0.5}$ model fine-tuned for the [LIBERO](https://libero-project.github.io/datasets) benchmark: gets state-of-the-art performance (see [LIBERO README](examples/libero/README.md)) | `gs://openpi-assets/checkpoints/pi05_libero`      |
|| $\pi_{0.5}$-DROID      | Inference / Fine-Tuning | $\pi_{0.5}$ model fine-tuned on the [DROID dataset](https://droid-dataset.github.io/) with [knowledge insulation](https://www.physicalintelligence.company/research/knowledge_insulation): fast inference and good language-following | `gs://openpi-assets/checkpoints/pi05_droid`      | -->

## è¿è¡Œé¢„è®­ç»ƒæ¨¡å‹æ¨ç†

### 1. [é‡è¦] ç¡¬ä»¶è®¾ç½®ä¸æ ‡å®š

1. æ ¹æ®æˆ‘ä»¬çš„[ç¡¬ä»¶æŒ‡å—](https://docs.google.com/document/d/1HUeM4Wlt4PyINoEwci-hxm8U9wAxiPMgR3sHyaOAsck/edit?tab=t.0#heading=h.sbdalb8w1kk1)è·å–éƒ¨ç½²ç¡¬ä»¶ã€‚

2. è®¾ç½®æœºå™¨äºº

- 2.1 è®¾ç½® UR5e  
   - è·å– IP åœ°å€å¹¶æ›´æ–° [configs/robots/eval_bimanual_ur5e_config.yaml](configs/robots/eval_bimanual_ur5e_config.yaml) ä¸­çš„ `robots/robot_ip`ã€‚  
  - åœ¨å®‰è£… > è´Ÿè½½ä¸­  
    - å°†è´¨é‡è®¾ç½®ä¸º 0.82 kg  
    - å°†æƒ¯æ€§çŸ©é˜µè®¾ç½®ä¸º  
      ```python
      [0.001106, 0, 0,
       0, 0.001106, 0,
       0, 0, 0.001106]
      ```
    - å°†é€Ÿåº¦è®¾ç½®ä¸º 30%ï¼ˆæ¨èï¼‰
  
- 2.2 è®¾ç½® Franka FR3  
  - è·å– IP åœ°å€å¹¶æ›´æ–° [configs/robots/eval_bimanual_fr3_config.yaml](configs/robots/eval_bimanual_fr3_config.yaml) ä¸­çš„ `robots/robot_ip`ã€‚  
  - åœ¨ Franka ç•Œé¢ç½‘ç«™ä¸Š  
    - å°†å¤¹çˆªè´¨é‡è®¾ç½®ä¸º 1.9 kg  
    - å°†æƒ¯æ€§å¼ é‡è®¾ç½®ä¸º  
      ```python
      [0.001, 0, 0,
       0, 0.001, 0,
       0, 0, 0.001]
      ```

3. è®¾ç½®ç›¸æœº
   * ä» [æµ·åº·æœºå™¨äººç½‘ç«™](https://www.hikrobotics.com/cn/machinevision/service/download/?module=0) ä¸‹è½½ SDK å¹¶å®‰è£…æ‰€æœ‰ `.deb` æ–‡ä»¶ã€‚
   * è¿è¡Œ `cd /opt/MVS/bin && ./MVS.sh`ã€‚é€‰æ‹©æ‚¨çš„ç›¸æœºï¼Œå¹¶å°†é‡‡é›†æ§åˆ¶ -> æ›å…‰æ—¶é—´è®¾ç½®ä¸º 20000ã€‚
  
4. å°†æœºå™¨äººæ ‡å®šåˆ°è·Ÿè¸ªå™¨çš„ TCP ç©ºé—´
 * æŒ‰ç…§[ç¡¬ä»¶æŒ‡å—](https://docs.google.com/document/d/1HUeM4Wlt4PyINoEwci-hxm8U9wAxiPMgR3sHyaOAsck/edit?tab=t.0#heading=h.sbdalb8w1kk1)ä¸­çš„æ ‡å®šè®¾ç½®è¯´æ˜è¿›è¡Œæ“ä½œã€‚
 * æ ¹æ®æ­¤[æ•™ç¨‹](https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit?tab=t.0#heading=h.yxlxo67jgfyx)è®¾ç½® Vive Tracker -> è½¯ä»¶è®¾ç½®æ•™ç¨‹ -> VIVE tracker è®¾ç½®
 * è¿è¡Œä»¥ä¸‹ä»£ç å°†æœºå™¨äºº TCP ç©ºé—´æ ‡å®šåˆ°è·Ÿè¸ªå™¨ç©ºé—´ã€‚
 * é‡è¦æç¤ºï¼šæ­¤è„šæœ¬ä¼šä½¿æœºå™¨äººæ‰§è¡Œå°å¹…åº¦çš„æ­£å¼¦è¿åŠ¨ï¼›åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿æœºå™¨äººå¤„äºå®‰å…¨ä½ç½®ï¼Œå·¥ä½œç©ºé—´å†…æ²¡æœ‰éšœç¢ç‰©ã€‚
    ```bash
    python deploy/calibration/calibrate_franka.py --franka_ip <your_franka_server_ip> --franka_port <your_franka_server_port> # å¦‚æœä½¿ç”¨ Franka Research 3
    # æˆ–è€…
    python deploy/calibration/calibrate_ur5e.py --ur5e_ip <your_ur5e_ip> # å¦‚æœä½¿ç”¨ UR5e
    ```
  * æ ‡å®šåï¼Œè¿è¡Œä»¥ä¸‹è„šæœ¬è·å–æ ‡å®šçŸ©é˜µï¼š
    ```bash
    python deploy/calibration/compute_calibration_matrix.py
    ```
    ç„¶åå°†æ ‡å®šçŸ©é˜µç²˜è´´åˆ° `eval_bimanual_ur5e_config.yaml` çš„ `tx_tracker_to_tcp`ï¼ˆå¦‚æœä½¿ç”¨ FR3ï¼Œåˆ™ç²˜è´´åˆ° `eval_bimanual_fr3_config.yaml` çš„ `tx_tracker_to_tcp`ï¼‰ã€‚

### 2. è¿è¡Œæ¨ç†

æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹å¯ä»¥ç”¨å‡ è¡Œä»£ç è¿è¡Œï¼ˆè¿™é‡Œä»¥æˆ‘ä»¬çš„ [RDT2-VQ æ¨¡å‹](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ) ä¸ºä¾‹ï¼‰ï¼š
```python
import torch
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

from vqvae import MultiVQVAE
from models.normalizer import LinearNormalizer
from utils import batch_predict_action

# å‡è®¾ä½¿ç”¨ gpu 0
device = "cuda:0"


processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct")
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "robotics-diffusion-transformer/RDT2-VQ",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    device_map=device
).eval()
vae = MultiVQVAE.from_pretrained("robotics-diffusion-transformer/RVQActionTokenizer").eval()
vae = vae.to(device=device, dtype=torch.float32)

valid_action_id_length = (
    vae.pos_id_len + vae.rot_id_len + vae.grip_id_len
)
# TODO: ä¿®æ”¹ä¸ºæ‚¨è‡ªå·±ä¸‹è½½çš„å½’ä¸€åŒ–å™¨è·¯å¾„
# ä» http://ml.cs.tsinghua.edu.cn/~lingxuan/rdt2/umi_normalizer_wo_downsample_indentity_rot.pt ä¸‹è½½
normalizer = LinearNormalizer.from_pretrained("umi_normalizer_wo_downsample_indentity_rot.pt")  # 

result = batch_predict_action(
    model,
    processor,
    vae,
    normalizer,
    examples=[
        {
            "obs": {
                # æ³¨æ„ï¼šéµå¾ª UMI çš„è®¾ç½®ï¼Œcamera0_rgb ç”¨äºå³è‡‚ï¼Œcamera1_rgb ç”¨äºå·¦è‡‚
                "camera0_rgb": ..., # å³è‡‚ RGB å›¾åƒï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (1, 384, 384, 3)ï¼Œæ•°æ®ç±»å‹ä¸º np.uint8
                "camera1_rgb": ..., # å·¦è‡‚ RGB å›¾åƒï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (1, 384, 384, 3)ï¼Œæ•°æ®ç±»å‹ä¸º np.uint8
            },
            "meta": {
                "num_camera": 2
            }
        },
        ...,    # æˆ‘ä»¬æ”¯æŒæ‰¹é‡æ¨ç†ï¼Œå› æ­¤æ‚¨å¯ä»¥ä¼ é€’ä¸€ä¸ªç¤ºä¾‹åˆ—è¡¨
    ],
    valid_action_id_length=valid_action_id_length,
    apply_jpeg_compression=True,
    # ç”±äºæ¨¡å‹ä¸»è¦ä½¿ç”¨ jpeg å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å»ºè®®å¼€å¯æ­¤é€‰é¡¹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
    instruction="Pick up the apple."
    # æˆ‘ä»¬å»ºè®®ä½¿ç”¨æ ¼å¼ä¸º"åŠ¨è¯ + å¯¹è±¡"çš„æŒ‡ä»¤ï¼Œé¦–å­—æ¯å¤§å†™å¹¶ä»¥å¥å·ç»“å°¾
)

# ä»ç¤ºä¾‹ 0 è·å–é¢„æµ‹çš„åŠ¨ä½œ
action_chunk = result["action_pred"][0] # torch.FloatTensor æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (24, 20)ï¼Œæ•°æ®ç±»å‹ä¸º torch.float32
# action_chunk (T, D)ï¼Œå…¶ä¸­ T=24ï¼ŒD=20
#   T=24ï¼šæˆ‘ä»¬çš„ action_chunk åœ¨ fps=30 ä¸‹é¢„æµ‹æœªæ¥ 0.8 ç§’ï¼Œå³ 24 å¸§
#   D=20ï¼šéµå¾ª UMI çš„è®¾ç½®ï¼Œæˆ‘ä»¬ä»å³åˆ°å·¦é¢„æµ‹åŒè‡‚çš„åŠ¨ä½œ
#   - [0-2]ï¼šå³è‡‚æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® x, y, zï¼ˆå•ä½ï¼šç±³ï¼‰
#   - [3-8]ï¼šå³è‡‚æœ«ç«¯æ‰§è¡Œå™¨æ—‹è½¬ï¼Œ6D æ—‹è½¬è¡¨ç¤º
#   - [9]ï¼šå³è‡‚å¤¹çˆªå®½åº¦ï¼ˆå•ä½ï¼šç±³ï¼‰
#   - [10-12]ï¼šå·¦è‡‚æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® x, y, zï¼ˆå•ä½ï¼šç±³ï¼‰
#   - [13-18]ï¼šå·¦è‡‚æœ«ç«¯æ‰§è¡Œå™¨æ—‹è½¬ï¼Œ6D æ—‹è½¬è¡¨ç¤º
#   - [19]ï¼šå·¦è‡‚å¤¹çˆªå®½åº¦ï¼ˆå•ä½ï¼šç±³ï¼‰

# å°†å¤¹çˆªå®½åº¦ä» [0, 0.088] é‡æ–°ç¼©æ”¾åˆ° [0, 0.1]
for robot_idx in range(2):
    action_chunk[:, robot_idx * 10 + 9] = action_chunk[:, robot_idx * 10 + 9] / 0.088 * 0.1
```

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æµ‹è¯• [RDT2-FM](https://huggingface.co/robotics-diffusion-transformer/RDT2-FM)ï¼š
```python
# åœ¨æˆ‘ä»¬çš„ä»“åº“æ ¹ç›®å½•ä¸‹è¿è¡Œ
import yaml

from models.rdt_inferencer import RDTInferencer


with open("configs/rdt/post_train.yaml", "r") as f:
  model_config = yaml.safe_load(f)

model = RDTInferencer(
  config=model_config,
  pretrained_path="robotics-diffusion-transformer/RDT2-FM",
  # TODO: ä¿®æ”¹ `normalizer_path` ä¸ºæ‚¨è‡ªå·±ä¸‹è½½çš„å½’ä¸€åŒ–å™¨è·¯å¾„
  # ä» http://ml.cs.tsinghua.edu.cn/~lingxuan/rdt2/umi_normalizer_wo_downsample_indentity_rot.pt ä¸‹è½½
  normalizer_path="umi_normalizer_wo_downsample_indentity_rot.pt",  
  pretrained_vision_language_model_name_or_path="robotics-diffusion-transformer/RDT2-VQ", # ä½¿ç”¨ RDT2-VQ ä½œä¸º VLM éª¨å¹²ç½‘ç»œ
  device="cuda:0",
  dtype=torch.bfloat16,
)

result = model.step(
    observations={
        'images': {
            'left_stereo': ..., # å·¦è‡‚ RGB å›¾åƒï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (384, 384, 3)ï¼Œæ•°æ®ç±»å‹ä¸º np.uint8
            'right_stereo': ..., # å³è‡‚ RGB å›¾åƒï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (384, 384, 3)ï¼Œæ•°æ®ç±»å‹ä¸º np.uint8
        },
        # å½“å‰ä½¿ç”¨é›¶è¾“å…¥å½“å‰çŠ¶æ€
        # ä¿ç•™è¾“å…¥æ¥å£ä»¥ä¾¿æœªæ¥å¾®è°ƒ
        'state': np.zeros(model_config["common"]["state_dim"]).astype(np.float32)
    },
    instruction="Pick up the apple." # è¯­è¨€æŒ‡ä»¤
    # æˆ‘ä»¬å»ºè®®ä½¿ç”¨æ ¼å¼ä¸º"åŠ¨è¯ + å¯¹è±¡"çš„æŒ‡ä»¤ï¼Œé¦–å­—æ¯å¤§å†™å¹¶ä»¥å¥å·ç»“å°¾
)


# ç›¸å¯¹åŠ¨ä½œå—ï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (24, 20)ï¼Œæ•°æ®ç±»å‹ä¸º np.float32
# æ ¼å¼ä¸ RDT2-VQ ç›¸åŒ
action_chunk = result.detach().cpu().numpy()

# å°†å¤¹çˆªå®½åº¦ä» [0, 0.088] é‡æ–°ç¼©æ”¾åˆ° [0, 0.1]
for robot_idx in range(2):
    action_chunk[:, robot_idx * 10 + 9] = action_chunk[:, robot_idx * 10 + 9] / 0.088 * 0.1
```

<!-- You can also test this out in the [example notebook](examples/inference.ipynb). -->

æˆ‘ä»¬æä¾›äº†åœ¨ [åŒè‡‚ UR5e](examples/ur5e/README.md) å’Œ [åŒè‡‚ Franka Research 3](examples/fr3/README.md) æœºå™¨äººä¸Šè¿è¡Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹æ¨ç†çš„è¯¦ç»†åˆ†æ­¥ç¤ºä¾‹ã€‚

é‡è¦æç¤ºï¼šå¦‚æœåœ¨æ£€æŸ¥æ‰€æœ‰è®¾ç½®ã€é…ç½®å’Œæ ‡å®šåï¼Œæ¨ç†æˆåŠŸç‡ä»ç„¶è¾ƒä½ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[éƒ¨ç½²æŠ€å·§](./examples/DEPLOYMENT_TIPS.md)å¯»æ±‚å¸®åŠ©ã€‚

<!-- **Remote Inference**: We provide [examples and code](docs/remote_inference.md) for running inference of our models **remotely**: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate. -->

<!-- **Test inference without a robot**: We provide a [script](examples/simple_client/README.md) for testing inference without a robot. This script will generate a random observation and run inference with the model. See [here](examples/simple_client/README.md) for more details. -->


## åœ¨æ‚¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹

æˆ‘ä»¬å°†ä»¥åœ¨ [åŒè‡‚ UR5e ç¤ºä¾‹æ•°æ®é›†](https://huggingface.co/datasets/robotics-diffusion-transformer/BimanualUR5eExample) ä¸Šå¾®è°ƒ RDT2 æ¨¡å‹ä¸ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•åœ¨æ‚¨è‡ªå·±çš„æ•°æ®ä¸Šå¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å°†è§£é‡Šä¸‰ä¸ªæ­¥éª¤ï¼š
1. å°†æ‚¨çš„æ•°æ®è½¬æ¢ä¸º [webdataset](https://github.com/webdataset/webdataset) åˆ†ç‰‡ï¼ˆæˆ‘ä»¬ä½¿ç”¨æ­¤æ ¼å¼è¿›è¡Œè®­ç»ƒä»¥å®ç°é«˜æ•ˆ IOï¼‰
2. å®šä¹‰è®­ç»ƒé…ç½®
3. è¿è¡Œè®­ç»ƒ

### 1. å°†æ•°æ®è½¬æ¢ä¸º WebDataset åˆ†ç‰‡

<!-- We provide example scripts for converting assumed data sturcture to a webdataset dataset in [`data/preprocess/robot`](data/preprocess/robot) with detailed [guidelines](data/preprocess/robot/README.md). You can easily modify it to convert your own data!  -->
æ‚¨åº”è¯¥å°†æ•°æ®è½¬æ¢ä¸ºå¤„ç†åçš„ webdataset åˆ†ç‰‡ï¼Œå…·æœ‰ä»¥ä¸‹ç»“æ„ï¼š

```bash 
shard-000000.tar
â”œâ”€â”€ 0.image.jpg   # åŒç›®ï¼ˆå·¦æ‰‹è…•ç›¸æœº + å³æ‰‹è…•ç›¸æœºï¼‰RGB å›¾åƒï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (384, 768, 3)ï¼Œæ•°æ®ç±»å‹ä¸º np.uint8
â”œâ”€â”€ 0.action.npy  # ç›¸å¯¹åŠ¨ä½œå—ï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (24, 20)ï¼Œæ•°æ®ç±»å‹ä¸º np.float32
â”œâ”€â”€ 0.action_token.npy # å¯¹åº”çš„åŠ¨ä½œæ ‡è®°ï¼Œnp.ndarray æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (27,)ï¼Œå–å€¼èŒƒå›´ 0 åˆ° 1024ï¼Œæ•°æ®ç±»å‹ä¸º np.int16
â”œâ”€â”€ 0.meta.json # å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬é”® `sub_task_instruction_key`ï¼Œç”¨äºä» `instructions.json` ä¸­ç´¢å¼•å¯¹åº”çš„æŒ‡ä»¤
â”œâ”€â”€ 1.image.jpg
â”œâ”€â”€ 1.action.npy
â”œâ”€â”€ 1.action_token.npy
â”œâ”€â”€ 1.meta.json
â”œâ”€â”€ ...
shard-000001.tar
shard-000002.tar
...
```

æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ Hugging Face ä¸Šæä¾›äº†ä½¿ç”¨åŒè‡‚ UR5e æ”¶é›†çš„å¤„ç†åçš„[ç¤ºä¾‹æ•°æ®](https://huggingface.co/datasets/robotics-diffusion-transformer/BimanualUR5eExample)ã€‚æ‚¨å¯ä»¥ä¸‹è½½å¹¶ç›´æ¥ä½¿ç”¨ã€‚

### 2. å®šä¹‰è®­ç»ƒé…ç½®å¹¶è¿è¡Œè®­ç»ƒ

æŒ‰ç…§ [`configs/datasets/example.yaml`](configs/datasets/example.yaml) ä¸­çš„æ ¼å¼å®šä¹‰æ‚¨çš„æ•°æ®é›†é…ç½®
```yaml
# åœ¨æ­¤å®šä¹‰æ‚¨çš„æ•°æ®é›†åç§°
name: <your_dataset_name> # ä¾‹å¦‚ï¼šbimanual/ur_example
type: single
shards_dir: <your_shards_dir> # ä¾‹å¦‚ï¼š/ssd/rdt2/bimanual_fold_cloth/shards 
kwargs:
  instruction_path: <your_instruction_path> # ä¾‹å¦‚ï¼š/ssd/rdt2/ur_example/instruction.json
  normalizer_path: <your_normalizer_path> # ä¾‹å¦‚ï¼š/ssd/rdt2/umi_normalizer_wo_downsample_indentity_rot.pt
```

å¯¹äºæä¾›çš„ç¤ºä¾‹æ•°æ®ï¼Œå…¶å¯¹åº”çš„é…ç½®åœ¨ [`configs/datasets/example.yaml`](configs/datasets/example.yaml) ä¸­ã€‚è¯·è®°ä½å°† `<root_dir>` å’Œ `<path_to_normalizer>` æ›¿æ¢ä¸ºæ‚¨è‡ªå·±ä¸‹è½½çš„è·¯å¾„ã€‚

### 3. è¿è¡Œè®­ç»ƒ

#### RDT2-VQ

ç›®å‰ï¼Œæˆ‘ä»¬æ”¯æŒä»¥ä¸‹å¾®è°ƒæ–¹æ³•ï¼š

- DeepSpeed è®­ç»ƒ
- LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è®­ç»ƒ

ç”±äº RDT2-VQ åŸºäº Qwen2.5-VLï¼Œæ‚¨å¯ä»¥è‡ªç”±åº”ç”¨å…¶ä»–æŠ€æœ¯ï¼ˆä¾‹å¦‚ fsdpã€é‡åŒ–ï¼‰ï¼Œéµå¾ª Qwen2.5-VL çš„å¾®è°ƒå®è·µã€‚
æˆ‘ä»¬æä¾›äº†[å…¨å‚æ•°](scripts/finetune_full_param.sh)å’Œ [LoRA](scripts/finetune_lora.sh) å¾®è°ƒçš„ç¤ºä¾‹å¾®è°ƒè„šæœ¬ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›è„šæœ¬æ¥å¯åŠ¨è‡ªå·±çš„è®­ç»ƒã€‚

ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæˆ‘ä»¬è¯¦ç»†è§£é‡Šäº†ä½¿ç”¨ç¤ºä¾‹æ•°æ®çš„å…¨å‚æ•°å¾®è°ƒè„šæœ¬ï¼ˆ[`scripts/finetune_full_param.sh`](scripts/finetune_full_param.sh)ï¼‰çš„é€è¡Œè¯´æ˜ï¼š

```bash
# åœ¨æ­¤å®šä¹‰æ‚¨çš„ç¯å¢ƒè®¾ç½®
# ä¾‹å¦‚ï¼šncclã€ç½‘ç»œã€ä»£ç†ç­‰

TASK="bimanual-ur5e-example"  # åœ¨æ­¤å®šä¹‰æ‚¨çš„ä»»åŠ¡åç§°
DATASET_CONFIG_PATH="configs/datasets/example.yaml"  # åœ¨æ­¤å®šä¹‰æ‚¨çš„æ•°æ®é›†é…ç½®è·¯å¾„

export TOKENIZER_ID="Qwen/Qwen2.5-VL-7B-Instruct"
export VAE_ID="robotics-diffusion-transformer/RVQActionTokenizer" 
export MODEL_ID="robotics-diffusion-transformer/RDT2-VQ"
export OUTPUT_DIR="outputs/vqvla-sft-${TASK}" # åœ¨æ­¤å®šä¹‰æ‚¨çš„è¾“å‡ºç›®å½•

if [ ! -d "$OUTPUT_DIR" ]; then
    mkdir "$OUTPUT_DIR"
    echo "Folder '$OUTPUT_DIR' created"
else
    echo "Folder '$OUTPUT_DIR' already exists"
fi

accelerate launch main.py \
    --deepspeed="scripts/zero1.json" \  # DeepSpeed é…ç½®æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹ä¸ºä½¿ç”¨å…¶ä»–åˆ†ç‰‡ç­–ç•¥
    --tokenizer_name=$TOKENIZER_ID \
    --vae_name=$VAE_ID \
    --pretrained_model_name_or_path=$MODEL_ID \
    --output_dir=$OUTPUT_DIR \
    --train_batch_size=64 \
    --eval_batch_size=32 \
    --max_train_steps=10000 \ # æˆ‘ä»¬å»ºè®®è®­ç»ƒå°‘äº 5 ä¸ª epoch ä»¥é¿å…è¿‡æ‹Ÿåˆï¼Œ
                              # æ‚¨åº”è¯¥æ ¹æ®æ•°æ®ä¼°ç®—æ­¥æ•°å¹¶ç›¸åº”è®¾ç½®
    --eval_strategy="no" \
    --logging_steps=25 \
    --checkpoints_total_limit=20 \
    --checkpointing_step=1000 \
    --lr_scheduler="cosine" \
    --learning_rate=1e-5 \
    --mixed_precision="bf16" \
    --dataloader_num_workers=16 \
    --gradient_checkpointing \
    --log_level="info" \
    --report_to="wandb" \
    --lr_warmup_steps=500 \
    --dataset=$DATASET_CONFIG_PATH \
    --image_corruption \ # æˆ‘ä»¬å»ºè®®å¼€å¯æ­¤é€‰é¡¹ä»¥è·å¾—æ›´å¥½çš„è§†è§‰é²æ£’æ€§
    --use_default_collate_fn_for_eval
```

å°½ç®¡æˆ‘ä»¬çš„ RVQ åœ¨æ‰‹æŒå¤¹çˆªæ•°æ®å’ŒçœŸå®æœºå™¨äººæ•°æ®ä¹‹é—´éƒ½è¡¨ç°å‡ºé«˜åº¦çš„æ³›åŒ–æ€§ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨æˆ‘ä»¬çš„æ®‹å·® VQ ä½œä¸ºåŠ¨ä½œæ ‡è®°å™¨åœ¨æ‚¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œ
æˆ‘ä»¬çœŸè¯šåœ°å»ºè®®æ‚¨é¦–å…ˆæ£€æŸ¥æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯æ˜¯å¦åœ¨æˆ‘ä»¬æ®‹å·® VQ çš„èŒƒå›´å†…ï¼Œç„¶åæµ‹è¯•æ•°æ®çš„é‡å»ºè¯¯å·®ã€‚

<!-- **Note:** We provide a [script]() for compute normalization statistics fo action normalization for bound violation check. This can be beneficial if you are fine-tuning to a new task on a robot.  -->

#### RDT2-FM

ç›®å‰ï¼Œæˆ‘ä»¬æ”¯æŒä½¿ç”¨ DeepSpeed å¾®è°ƒ RDT2-FM çš„åŠ¨ä½œä¸“å®¶ï¼šæˆ‘ä»¬æä¾›äº†[å…¨å‚æ•°åŠ¨ä½œä¸“å®¶](scripts/finetune_rdt.sh)å¾®è°ƒçš„ç¤ºä¾‹å¾®è°ƒè„šæœ¬ã€‚åœ¨æŒ‡å®šæ‚¨è‡ªå·±çš„[æ•°æ®é›†é…ç½®è·¯å¾„](scripts/finetune_rdt.sh#L20)å¹¶å°†[å…¨å‚æ•°åŠ¨ä½œä¸“å®¶](scripts/finetune_rdt.sh#L42)ä¸­çš„ `<repository-path>` æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„ä»“åº“è·¯å¾„åï¼Œæ‚¨å¯ä»¥ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ¥å¯åŠ¨è®­ç»ƒã€‚

### ç²¾åº¦è®¾ç½®

ä¸åŒæ¨¡å‹æœ‰ç‰¹å®šçš„ç²¾åº¦è®¾ç½®ï¼š

**åŠ¨ä½œæ ‡è®°å™¨ï¼ˆæ®‹å·® VQï¼‰ï¼š**

ç”±äºæ®‹å·® VQ çš„å°ºå¯¸éå¸¸å°ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­éƒ½ä½¿ç”¨ `float32`ã€‚

**RDT VLM ([RDT2-VQ](https://huggingface.co/robotics-diffusion-transformer/RDT2-VQ))ï¼š**

éµå¾ª Qwen2.5-VLï¼Œä½¿ç”¨å®Œæ•´çš„ `bfloat16`ï¼ˆé»˜è®¤ï¼‰ã€‚æ‚¨å¯ä»¥éµå¾ª [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL) çš„å®è·µï¼Œé€šè¿‡åº”ç”¨æ··åˆç²¾åº¦æˆ–é‡åŒ–ç­‰æŠ€æœ¯æ¥è°ƒæ•´ç²¾åº¦ã€‚

<!-- **RDT Action Expert ([RDT2-FM](robotics-diffusion-transformer/RDT2-FM) \& [RDT2-FM-UltraFast](robotics-diffusion-transformer/RDT2-FM-UltraFast)):** -->
**RDT åŠ¨ä½œä¸“å®¶ ([RDT2-FM](robotics-diffusion-transformer/RDT2-FM))ï¼š**

åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­éƒ½ä½¿ç”¨å®Œæ•´çš„ `bfloat16`ã€‚

## æ•…éšœæ’é™¤

æˆ‘ä»¬å°†åœ¨æ­¤æ”¶é›†å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚å¦‚æœæ‚¨é‡åˆ°é—®é¢˜ï¼Œè¯·å…ˆåœ¨æ­¤å¤„æŸ¥çœ‹ã€‚å¦‚æœæ‰¾ä¸åˆ°è§£å†³æ–¹æ¡ˆï¼Œè¯·åœ¨ä»“åº“ä¸Šæäº¤é—®é¢˜ï¼ˆè¯·å‚é˜…[æ­¤å¤„](CONTRIBUTING.md)äº†è§£æŒ‡å—ï¼‰ã€‚

| é—®é¢˜                                     | è§£å†³æ–¹æ¡ˆ                                                                                                                                                                                   |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 
| ğŸš§ In progress ğŸš§ | ğŸš§ In progress ğŸš§ |

## Citation

If you find our work helpful, please cite us:

```bibtex
@misc{liu2026rdt2exploringscalinglimit,
      title={RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization}, 
      author={Songming Liu and Bangguo Li and Kai Ma and Lingxuan Wu and Hengkai Tan and Xiao Ouyang and Hang Su and Jun Zhu},
      year={2026},
      eprint={2602.03310},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2602.03310}, 
}
```
Thank you!
